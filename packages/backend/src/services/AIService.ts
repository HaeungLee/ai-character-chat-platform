// í†µí•© AI ì„œë¹„ìŠ¤
import { OpenAIService, OpenAIConfig } from './ai/OpenAIService'
import { OpenRouterService, OpenRouterConfig, OPENROUTER_MODELS } from './ai/OpenRouterService'
import { ReplicateService, ReplicateConfig } from './ai/ReplicateService'
import { StabilityAIService, StabilityConfig } from './ai/StabilityAIService'
import { UsageTrackingService, UsageRecord, AIProvider as BillingAIProvider } from './billing'
import { logger } from '../utils/logger'

export interface AIServiceConfig {
  openai?: OpenAIConfig
  openrouter?: OpenRouterConfig
  replicate?: ReplicateConfig
  stability?: StabilityConfig
  usageTracker?: UsageTrackingService
}

// AI í”„ë¡œë°”ì´ë” íƒ€ì…
export type AIProvider = 'openai' | 'openrouter'

// ì±„íŒ… ì˜µì…˜
export interface ChatOptions {
  provider?: AIProvider
  model?: string
  temperature?: number
  maxTokens?: number
  nsfwMode?: boolean  // ê²€ì—´ í•´ì œ ëª¨ë“œ
}

export interface Character {
  id: string
  name: string
  personality: string
  systemPrompt: string
  temperature?: number
  avatar?: string
}

export interface ChatMessage {
  role: 'system' | 'user' | 'assistant'
  content: string
  timestamp?: Date
}

export class AIService {
  private openai?: OpenAIService
  private openrouter?: OpenRouterService
  private replicate?: ReplicateService
  private stability?: StabilityAIService
  private usageTracker?: UsageTrackingService
  private defaultProvider: AIProvider = 'openai'

  constructor(config: AIServiceConfig) {
    if (config.openai) {
      this.openai = new OpenAIService(config.openai)
    }
    if (config.openrouter) {
      this.openrouter = new OpenRouterService(config.openrouter)
    }
    if (config.replicate) {
      this.replicate = new ReplicateService(config.replicate)
    }
    if (config.stability) {
      this.stability = new StabilityAIService(config.stability)
    }
    if (config.usageTracker) {
      this.usageTracker = config.usageTracker
    }

    // ê¸°ë³¸ í”„ë¡œë°”ì´ë” ì„¤ì • (OpenAI ìš°ì„ , ì—†ìœ¼ë©´ OpenRouter)
    if (this.openai) {
      this.defaultProvider = 'openai'
    } else if (this.openrouter) {
      this.defaultProvider = 'openrouter'
    }
  }

  /**
   * í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡œë°”ì´ë” ì¡°íšŒ
   */
  getDefaultProvider(): AIProvider {
    return this.defaultProvider
  }

  /**
   * UsageTracker ì„¤ì • (ë‚˜ì¤‘ì— ì£¼ì… ê°€ëŠ¥)
   */
  setUsageTracker(tracker: UsageTrackingService): void {
    this.usageTracker = tracker
  }

  /**
   * ğŸ†• ì‚¬ìš©ëŸ‰ ê¸°ë¡ í—¬í¼
   */
  private async recordUsage(
    provider: AIProvider,
    requestType: 'chat' | 'chat_stream' | 'embedding' | 'summarization',
    options?: {
      userId?: string
      characterId?: string
      chatId?: string
      latencyMs?: number
      isSuccess?: boolean
      errorMessage?: string
    }
  ): Promise<void> {
    if (!this.usageTracker) return

    try {
      let usage: { promptTokens: number; completionTokens: number; model: string } | null = null

      if (provider === 'openai' && this.openai?.lastUsage) {
        usage = this.openai.lastUsage
      } else if (provider === 'openrouter' && this.openrouter?.lastUsage) {
        usage = this.openrouter.lastUsage
      }

      if (usage && options?.userId) {
        await this.usageTracker.recordUsage({
          userId: options.userId,
          provider: provider as BillingAIProvider,
          model: usage.model,
          promptTokens: usage.promptTokens,
          completionTokens: usage.completionTokens,
          requestType,
          characterId: options.characterId,
          chatId: options.chatId,
          latencyMs: options.latencyMs,
          isSuccess: options.isSuccess ?? true,
          errorMessage: options.errorMessage,
        })
      }
    } catch (error) {
      logger.warn('ì‚¬ìš©ëŸ‰ ê¸°ë¡ ì‹¤íŒ¨:', error)
      // ì‚¬ìš©ëŸ‰ ê¸°ë¡ ì‹¤íŒ¨ëŠ” ë©”ì¸ ë¡œì§ì— ì˜í–¥ ì£¼ì§€ ì•ŠìŒ
    }
  }

  /**
   * í”„ë¡œë°”ì´ë” ì„ íƒ ë¡œì§
   * - nsfwMode: OpenRouter ê°•ì œ
   * - ëª…ì‹œì  provider ì§€ì •: í•´ë‹¹ í”„ë¡œë°”ì´ë” ì‚¬ìš©
   * - ê¸°ë³¸: defaultProvider ì‚¬ìš©
   */
  private selectProvider(options?: ChatOptions): AIProvider {
    if (options?.nsfwMode) {
      // NSFW ëª¨ë“œëŠ” OpenRouter í•„ìˆ˜
      if (!this.openrouter) {
        throw new Error('NSFW ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ OpenRouter API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.')
      }
      return 'openrouter'
    }

    if (options?.provider) {
      return options.provider
    }

    return this.defaultProvider
  }

  // ìºë¦­í„° ê¸°ë°˜ ì±„íŒ… ì‘ë‹µ ìƒì„±
  async generateCharacterResponse(
    character: Character,
    userMessage: string,
    conversationHistory: ChatMessage[] = [],
    options?: ChatOptions
  ): Promise<string> {
    const provider = this.selectProvider(options)

    try {
      const characterPrompt = {
        name: character.name,
        personality: character.personality,
        systemPrompt: character.systemPrompt,
        temperature: character.temperature || 0.7,
      }

      const messages: ChatMessage[] = conversationHistory.map(msg => ({
        role: msg.role,
        content: msg.content,
      }))

      if (provider === 'openrouter') {
        if (!this.openrouter) {
          throw new Error('OpenRouter ì„œë¹„ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
        }
        return await this.openrouter.generateCharacterResponse(
          characterPrompt,
          userMessage,
          messages,
          {
            model: options?.model,
            nsfwMode: options?.nsfwMode
          }
        )
      } else {
        if (!this.openai) {
          throw new Error('OpenAI ì„œë¹„ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
        }
        return await this.openai.generateCharacterResponse(
          characterPrompt,
          userMessage,
          messages
        )
      }
    } catch (error) {
      logger.error('ìºë¦­í„° ì‘ë‹µ ìƒì„± ì‹¤íŒ¨:', error)

      // Fallback: ë‹¤ë¥¸ í”„ë¡œë°”ì´ë”ë¡œ ì‹œë„
      if (provider === 'openai' && this.openrouter) {
        logger.info('OpenRouterë¡œ í´ë°± ì‹œë„')
        try {
          const characterPrompt = {
            name: character.name,
            personality: character.personality,
            systemPrompt: character.systemPrompt,
            temperature: character.temperature || 0.7,
          }
          const messages = conversationHistory.map(msg => ({
            role: msg.role,
            content: msg.content,
          }))
          return await this.openrouter.generateCharacterResponse(
            characterPrompt,
            userMessage,
            messages
          )
        } catch (fallbackError) {
          logger.error('í´ë°±ë„ ì‹¤íŒ¨:', fallbackError)
        }
      }

      return `${character.name}: ì£„ì†¡í•©ë‹ˆë‹¤. ì§€ê¸ˆì€ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.`
    }
  }

  // ğŸ†• ìºë¦­í„° ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (íƒ€ìê¸° íš¨ê³¼)
  async *generateCharacterResponseStream(
    character: Character,
    userMessage: string,
    conversationHistory: ChatMessage[] = [],
    options?: ChatOptions
  ): AsyncGenerator<string, void, unknown> {
    const provider = this.selectProvider(options)

    try {
      const characterPrompt = {
        name: character.name,
        personality: character.personality,
        systemPrompt: character.systemPrompt,
        temperature: character.temperature || 0.7,
      }

      const messages: ChatMessage[] = conversationHistory.map(msg => ({
        role: msg.role,
        content: msg.content,
      }))

      if (provider === 'openrouter') {
        if (!this.openrouter) {
          throw new Error('OpenRouter ì„œë¹„ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
        }
        yield* this.openrouter.generateCharacterResponseStream(
          characterPrompt,
          userMessage,
          messages,
          {
            model: options?.model,
            nsfwMode: options?.nsfwMode
          }
        )
      } else {
        if (!this.openai) {
          throw new Error('OpenAI ì„œë¹„ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
        }
        yield* this.openai.generateCharacterResponseStream(
          characterPrompt,
          userMessage,
          messages
        )
      }
    } catch (error) {
      logger.error('ìºë¦­í„° ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì‹¤íŒ¨:', error)
      throw error
    }
  }

  // ğŸ†• ì¼ë°˜ ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
  async *generateChatResponseStream(
    messages: ChatMessage[],
    options?: ChatOptions
  ): AsyncGenerator<string, void, unknown> {
    const provider = this.selectProvider(options)

    if (provider === 'openrouter') {
      if (!this.openrouter) {
        throw new Error('OpenRouter ì„œë¹„ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
      }
      yield* this.openrouter.generateChatResponseStream(messages, options)
    } else {
      if (!this.openai) {
        throw new Error('OpenAI ì„œë¹„ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
      }
      yield* this.openai.generateChatResponseStream(messages, options)
    }
  }

  // ì´ë¯¸ì§€ ìƒì„±
  async generateImage(
    prompt: string,
    options?: {
      model?: 'openai' | 'replicate' | 'stability'
      style?: string
      aspectRatio?: string
      negativePrompt?: string
    }
  ): Promise<string> {
    const model = options?.model || 'replicate'

    try {
      switch (model) {
        case 'openai':
          if (!this.openai) throw new Error('OpenAI ì„œë¹„ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
          return await this.openai.generateImage(prompt, {
            size: this.mapAspectRatioToOpenAI(options?.aspectRatio),
            style: options?.style as any,
          })

        case 'replicate':
          if (!this.replicate) throw new Error('Replicate ì„œë¹„ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
          const dimensions = this.parseAspectRatio(options?.aspectRatio)
          return await this.replicate.generateImage(prompt, {
            width: dimensions.width,
            height: dimensions.height,
            negativePrompt: options?.negativePrompt,
          })

        case 'stability':
          if (!this.stability) throw new Error('Stability AI ì„œë¹„ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
          const stabDimensions = this.parseAspectRatio(options?.aspectRatio)
          return await this.stability.generateImage(prompt, {
            width: stabDimensions.width,
            height: stabDimensions.height,
            negative_prompt: options?.negativePrompt,
            style_preset: options?.style,
          })

        default:
          throw new Error(`ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: ${model}`)
      }
    } catch (error) {
      logger.error(`${model} ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨:`, error)

      // ë‹¤ë¥¸ ì„œë¹„ìŠ¤ë¡œ í´ë°± ì‹œë„
      if (model !== 'replicate' && this.replicate) {
        logger.info('Replicateë¡œ í´ë°± ì‹œë„')
        try {
          const dimensions = this.parseAspectRatio(options?.aspectRatio)
          return await this.replicate.generateImage(prompt, {
            width: dimensions.width,
            height: dimensions.height,
            negativePrompt: options?.negativePrompt,
          })
        } catch (fallbackError) {
          logger.error('í´ë°±ë„ ì‹¤íŒ¨:', fallbackError)
        }
      }

      throw new Error('ì´ë¯¸ì§€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ëª¨ë¸ì„ ì‹œë„í•´ë³´ì„¸ìš”.')
    }
  }

  // ì¼ë°˜ ì±„íŒ… ì‘ë‹µ ìƒì„±
  async generateChatResponse(
    messages: ChatMessage[],
    options?: {
      model?: 'openai'
      temperature?: number
      maxTokens?: number
    }
  ): Promise<string> {
    if (!this.openai) {
      throw new Error('OpenAI ì„œë¹„ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
    }

    return await this.openai.generateChatResponse(messages, options)
  }

  // ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
  getServiceStatus(): {
    openai: boolean
    openrouter: boolean
    replicate: boolean
    stability: boolean
    defaultProvider: AIProvider
  } {
    return {
      openai: !!this.openai,
      openrouter: !!this.openrouter,
      replicate: !!this.replicate,
      stability: !!this.stability,
      defaultProvider: this.defaultProvider,
    }
  }

  // ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
  getAvailableModels(): {
    chat: string[]
    chatUncensored: string[]
    chatRoleplay: string[]
    chatFree: string[]
    image: string[]
  } {
    const status = this.getServiceStatus()

    const openrouterModels = this.openrouter?.getAvailableModels()

    return {
      chat: [
        ...(status.openai ? ['gpt-4', 'gpt-4o', 'gpt-3.5-turbo'] : []),
        ...(status.openrouter ? (openrouterModels?.premium || []) : []),
      ],
      chatUncensored: status.openrouter ? (openrouterModels?.uncensored || []) : [],
      chatRoleplay: status.openrouter ? (openrouterModels?.roleplay || []) : [],
      chatFree: status.openrouter ? (openrouterModels?.free || []) : [],
      image: [
        ...(status.openai ? ['openai/dall-e-3'] : []),
        ...(status.replicate ? ['replicate/sdxl', 'replicate/sdxl-turbo'] : []),
        ...(status.stability ? ['stability/sdxl', 'stability/core'] : []),
      ],
    }
  }

  // OpenRouter í¬ë ˆë”§ ì¡°íšŒ
  async getOpenRouterCredits(): Promise<{ usage: number; limit: number | null } | null> {
    if (!this.openrouter) return null
    try {
      return await this.openrouter.getCredits()
    } catch {
      return null
    }
  }

  // í—¬í¼ í•¨ìˆ˜ë“¤
  private mapAspectRatioToOpenAI(aspectRatio?: string): any {
    const ratioMap: Record<string, string> = {
      '1:1': '1024x1024',
      '16:9': '1792x1024',
      '9:16': '1024x1792',
      '4:3': '1024x768',
      '3:4': '768x1024',
    }
    return ratioMap[aspectRatio || '1:1'] || '1024x1024'
  }

  private parseAspectRatio(aspectRatio?: string): { width: number; height: number } {
    if (!aspectRatio) return { width: 1024, height: 1024 }

    const [widthStr, heightStr] = aspectRatio.split(':')
    const width = parseInt(widthStr)
    const height = parseInt(heightStr)

    if (isNaN(width) || isNaN(height)) return { width: 1024, height: 1024 }

    // ìµœëŒ€ í¬ê¸° ì œí•œ
    const maxSize = 1024
    let actualWidth = width
    let actualHeight = height

    if (width > height) {
      actualWidth = Math.min(width, maxSize)
      actualHeight = Math.round((height * actualWidth) / width)
    } else {
      actualHeight = Math.min(height, maxSize)
      actualWidth = Math.round((width * actualHeight) / height)
    }

    return { width: actualWidth, height: actualHeight }
  }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í•¨ìˆ˜
export function createAIService(config: AIServiceConfig): AIService {
  return new AIService(config)
}

// í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ìƒì„±
export function createAIServiceFromEnv(): AIService {
  const config: AIServiceConfig = {}

  if (process.env.OPENAI_API_KEY) {
    config.openai = {
      apiKey: process.env.OPENAI_API_KEY,
      organization: process.env.OPENAI_ORGANIZATION_ID,
      model: process.env.OPENAI_MODEL || 'gpt-4o',
      temperature: parseFloat(process.env.OPENAI_TEMPERATURE || '0.7'),
      maxTokens: parseInt(process.env.OPENAI_MAX_TOKENS || '1000'),
    }
  }

  if (process.env.OPENROUTER_API_KEY) {
    config.openrouter = {
      apiKey: process.env.OPENROUTER_API_KEY,
      siteUrl: process.env.OPENROUTER_SITE_URL || process.env.CORS_ORIGIN,
      siteName: process.env.OPENROUTER_SITE_NAME || 'AI Character Chat Platform',
      defaultModel: process.env.OPENROUTER_DEFAULT_MODEL || 'meta-llama/llama-3.2-3b-instruct:free',
    }
  }

  if (process.env.REPLICATE_API_TOKEN) {
    config.replicate = {
      apiToken: process.env.REPLICATE_API_TOKEN,
    }
  }

  if (process.env.STABILITY_API_KEY) {
    config.stability = {
      apiKey: process.env.STABILITY_API_KEY,
    }
  }

  // ì„¤ì • ë¡œê·¸
  const enabledServices = []
  if (config.openai) enabledServices.push('OpenAI')
  if (config.openrouter) enabledServices.push('OpenRouter')
  if (config.replicate) enabledServices.push('Replicate')
  if (config.stability) enabledServices.push('Stability')
  
  console.log(`ğŸ¤– AI Services enabled: ${enabledServices.join(', ') || 'None'}`)

  return new AIService(config)
}
